{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# HSTrans - Drug-Side Effect Prediction Training\n",
    "\n",
    "This notebook contains the complete training pipeline for the HSTrans model, a Transformer-based approach for predicting drug-side effect interactions.\n",
    "\n",
    "## üéØ Project Overview\n",
    "- **Model**: HSTrans (Hierarchical Transformer)\n",
    "- **Task**: Drug-Side Effect Prediction\n",
    "- **Architecture**: Cross-attention Transformer with substructure encoding\n",
    "- **Training**: 10-fold cross-validation\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Data Loading & Preparation](#data)\n",
    "3. [Model Architecture](#model)\n",
    "4. [Training Functions](#training)\n",
    "5. [Cross-Validation Training](#cv-training)\n",
    "6. [Results Analysis](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üõ†Ô∏è 1. Setup & Dependencies\n",
    "\n",
    "First, let's install all required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch>=1.9.0 numpy>=1.19.0 pandas>=1.2.0 scipy>=1.6.0 \n",
    "!pip install scikit-learn>=0.24.0 matplotlib>=3.3.0 rdkit-pypi>=2021.9.1 \n",
    "!pip install subword-nmt>=0.3.7 networkx>=2.5 tqdm\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import io\n",
    "from math import sqrt\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, \n",
    "                             precision_score, recall_score, accuracy_score)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch import optim\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# RDKit for chemistry\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "import codecs\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## üìÅ 2. Data Loading & Preparation\n",
    "\n",
    "Let's download the data and set up the file structure. We'll use Google Drive for persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create working directory\n",
    "WORK_DIR = '/content/HSTrans'\n",
    "DATA_DIR = f'{WORK_DIR}/data'\n",
    "SUB_DIR = f'{DATA_DIR}/sub'\n",
    "RESULTS_DIR = f'{WORK_DIR}/results'\n",
    "CHECKPOINTS_DIR = f'{WORK_DIR}/checkpoints'\n",
    "PREDICT_DIR = f'{WORK_DIR}/predictResult'\n",
    "\n",
    "# Create directories\n",
    "for path in [WORK_DIR, DATA_DIR, SUB_DIR, RESULTS_DIR, CHECKPOINTS_DIR, PREDICT_DIR]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Working directory: {WORK_DIR}\")\n",
    "print(\"‚úÖ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_data"
   },
   "outputs": [],
   "source": [
    "# Download data (replace with your actual data source)\n",
    "# For this example, we'll create a function to download from GitHub or other sources\n",
    "def download_data():\n",
    "    \"\"\"Download required data files\"\"\"\n",
    "    import gdown\n",
    "    \n",
    "    # Replace these with your actual file IDs or URLs\n",
    "    data_files = {\n",
    "        'raw_frequency_750.mat': 'YOUR_FILE_ID',  # Replace with actual ID\n",
    "        'drug_SMILES_750.csv': 'YOUR_FILE_ID',   # Replace with actual ID  \n",
    "        'mask_mat_750.mat': 'YOUR_FILE_ID',      # Replace with actual ID\n",
    "        'side_effect_label_750.mat': 'YOUR_FILE_ID', # Replace with actual ID\n",
    "        'drug_side.pkl': 'YOUR_FILE_ID',         # Replace with actual ID\n",
    "        'drug_codes_chembl_freq_1500.txt': 'YOUR_FILE_ID', # Replace with actual ID\n",
    "        'subword_units_map_chembl_freq_1500.csv': 'YOUR_FILE_ID' # Replace with actual ID\n",
    "    }\n",
    "    \n",
    "    print(\"üì• Downloading data files...\")\n",
    "    for filename, file_id in data_files.items():\n",
    "        if not os.path.exists(f'{DATA_DIR}/{filename}'):\n",
    "            try:\n",
    "                url = f'https://drive.google.com/uc?id={file_id}'\n",
    "                gdown.download(url, f'{DATA_DIR}/{filename}', quiet=False)\n",
    "                print(f\"‚úÖ Downloaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not download {filename}: {e}\")\n",
    "                print(f\"Please manually upload {filename} to {DATA_DIR}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {filename} already exists\")\n",
    "\n",
    "# Uncomment to download data\n",
    "# download_data()\n",
    "\n",
    "print(\"\\nüìã Please ensure the following files are in your data directory:\")\n",
    "print(\"   - raw_frequency_750.mat\")\n",
    "print(\"   - drug_SMILES_750.csv\")\n",
    "print(\"   - mask_mat_750.mat\")\n",
    "print(\"   - side_effect_label_750.mat\")\n",
    "print(\"   - drug_side.pkl\")\n",
    "print(\"   - drug_codes_chembl_freq_1500.txt\")\n",
    "print(\"   - subword_units_map_chembl_freq_1500.csv\")\n",
    "print(\"\\nüí° You can upload these files manually to Google Drive or use the download_data() function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_files"
   },
   "outputs": [],
   "source": [
    "# Check if data files exist\n",
    "required_files = [\n",
    "    'raw_frequency_750.mat',\n",
    "    'drug_SMILES_750.csv', \n",
    "    'mask_mat_750.mat',\n",
    "    'side_effect_label_750.mat',\n",
    "    'drug_side.pkl',\n",
    "    'drug_codes_chembl_freq_1500.txt',\n",
    "    'subword_units_map_chembl_freq_1500.csv'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    if os.path.exists(f'{DATA_DIR}/{file}'):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(missing_files)} files are missing. Please upload them to continue.\")\n",
    "else:\n",
    "    print(\"\\nüéâ All data files are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## üèóÔ∏è 3. Model Architecture\n",
    "\n",
    "Now let's define the HSTrans model architecture and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utils_functions"
   },
   "outputs": [],
   "source": [
    "# Utility functions for metrics\n",
    "def rmse(y, f):\n",
    "    return sqrt(((y - f) ** 2).mean())\n",
    "\n",
    "def mse(y, f):\n",
    "    return ((y - f) ** 2).mean()\n",
    "\n",
    "def pearson(y, f):\n",
    "    return np.corrcoef(y, f)[0, 1]\n",
    "\n",
    "def spearman(y, f):\n",
    "    return stats.spearmanr(y, f)[0]\n",
    "\n",
    "def MAE(y, f):\n",
    "    import sklearn\n",
    "    return sklearn.metrics.mean_absolute_error(y, f)\n",
    "\n",
    "print(\"üîß Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smiles_processing"
   },
   "outputs": [],
   "source": [
    "# SMILES processing functions\n",
    "def atom_features(atom):\n",
    "    HYB_list = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "                Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "                Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2,\n",
    "                Chem.rdchem.HybridizationType.UNSPECIFIED, Chem.rdchem.HybridizationType.OTHER]\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n",
    "                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n",
    "                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
    "                                           'Pt', 'Hg', 'Pb', 'Sm', 'Tc', 'Gd', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetExplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "                    one_of_k_encoding(atom.GetFormalCharge(), [-4, -3, -2, -1, 0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding(atom.GetHybridization(), HYB_list) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def load_drug_smile(file):\n",
    "    reader = pd.read_csv(file)\n",
    "    drug_dict = {}\n",
    "    drug_smile = []\n",
    "    \n",
    "    for idx, row in reader.iterrows():\n",
    "        name = str(row.iloc[0])\n",
    "        smile = str(row.iloc[1])\n",
    "        if name not in drug_dict:\n",
    "            pos = len(drug_dict)\n",
    "            drug_dict[name] = pos\n",
    "        drug_smile.append(smile)\n",
    "    \n",
    "    return drug_dict, drug_smile\n",
    "\n",
    "def drug2emb_encoder(smile):\n",
    "    vocab_path = f'{DATA_DIR}/drug_codes_chembl_freq_1500.txt'\n",
    "    sub_csv = pd.read_csv(f'{DATA_DIR}/subword_units_map_chembl_freq_1500.csv')\n",
    "\n",
    "    bpe_codes_drug = codecs.open(vocab_path)\n",
    "    dbpe = BPE(bpe_codes_drug, merges=-1, separator='')\n",
    "    idx2word_d = sub_csv['index'].values\n",
    "    words2idx_d = dict(zip(idx2word_d, range(0, len(idx2word_d))))\n",
    "\n",
    "    max_d = 50\n",
    "    t1 = dbpe.process_line(smile).split()\n",
    "    try:\n",
    "        i1 = np.asarray([words2idx_d[i] for i in t1])\n",
    "    except:\n",
    "        i1 = np.array([0])\n",
    "\n",
    "    l = len(i1)\n",
    "    if l < max_d:\n",
    "        i = np.pad(i1, (0, max_d - l), 'constant', constant_values=0)\n",
    "        input_mask = ([1] * l) + ([0] * (max_d - l))\n",
    "    else:\n",
    "        i = i1[:max_d]\n",
    "        input_mask = [1] * max_d\n",
    "\n",
    "    return i, np.asarray(input_mask)\n",
    "\n",
    "print(\"üß™ SMILES processing functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer_layers"
   },
   "outputs": [],
   "source": [
    "# Transformer architecture components\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_size, dropout_rate):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "print(\"üèóÔ∏è Transformer components loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attention_mechanisms"
   },
   "outputs": [],
   "source": [
    "# Attention mechanisms\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.query2 = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key2 = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value2 = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, fusion):\n",
    "        if fusion:\n",
    "            mixed_query_layer = self.query(hidden_states[0])\n",
    "            mixed_key_layer = self.key(hidden_states[0])\n",
    "            mixed_value_layer = self.value(hidden_states[0])\n",
    "\n",
    "            mixed_query_layer1 = self.query2(hidden_states[1])\n",
    "            mixed_key_layer1 = self.key2(hidden_states[1])\n",
    "            mixed_value_layer1 = self.value2(hidden_states[1])\n",
    "\n",
    "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "            key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "            value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "            query_layer1 = self.transpose_for_scores(mixed_query_layer1)\n",
    "            key_layer1 = self.transpose_for_scores(mixed_key_layer1)\n",
    "            value_layer1 = self.transpose_for_scores(mixed_value_layer1)\n",
    "\n",
    "            attention_scores = torch.matmul(query_layer, key_layer1.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "            attention_scores1 = torch.matmul(query_layer1, key_layer.transpose(-1, -2))\n",
    "            attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n",
    "            attention_scores1 = attention_scores1 + attention_mask\n",
    "            \n",
    "            attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "            attention_probs1 = nn.Softmax(dim=-1)(attention_scores1)\n",
    "\n",
    "            attention_probs = self.dropout(attention_probs)\n",
    "            attention_probs1 = self.dropout(attention_probs1)\n",
    "\n",
    "            context_layer = torch.matmul(attention_probs1, value_layer)\n",
    "            context_layer1 = torch.matmul(attention_probs, value_layer1)\n",
    "            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "            context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)\n",
    "\n",
    "            context_layer = context_layer.view(*new_context_layer_shape)\n",
    "            context_layer1 = context_layer1.view(*new_context_layer_shape1)\n",
    "\n",
    "            context_layer = torch.cat((context_layer.unsqueeze(0), context_layer1.unsqueeze(0)), 0)\n",
    "        else:\n",
    "            mixed_query_layer = self.query(hidden_states)\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "            key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "            value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "            attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "            attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "            context_layer = torch.matmul(attention_probs, value_layer)\n",
    "            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "print(\"üëÅÔ∏è Attention mechanisms loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encoder_layers"
   },
   "outputs": [],
   "source": [
    "# Encoder layers\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class SelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_dropout_prob):\n",
    "        super(SelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.self = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, fusion):\n",
    "        self_output = self.self(input_tensor, attention_mask, fusion)\n",
    "        if fusion:\n",
    "            input_tensor = torch.cat((input_tensor[0].unsqueeze(0), input_tensor[1].unsqueeze(0)), 0)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output\n",
    "\n",
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Output(nn.Module):\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob,\n",
    "                 hidden_dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Attention(hidden_size, num_attention_heads,\n",
    "                                   attention_probs_dropout_prob, hidden_dropout_prob)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, fusion):\n",
    "        attention_output = self.attention(hidden_states, attention_mask, fusion)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class Encoder_MultipleLayers(nn.Module):\n",
    "    def __init__(self, n_layer, hidden_size, intermediate_size,\n",
    "                 num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Encoder_MultipleLayers, self).__init__()\n",
    "        layer = Encoder(hidden_size, intermediate_size, num_attention_heads,\n",
    "                        attention_probs_dropout_prob, hidden_dropout_prob)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, fusion, output_all_encoded_layers=True):\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask, fusion)\n",
    "        return hidden_states\n",
    "\n",
    "print(\"üîÑ Encoder layers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hstrans_model"
   },
   "outputs": [],
   "source": [
    "# HSTrans Model\n",
    "class Trans(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Trans, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        input_dim_drug = 2586\n",
    "        transformer_emb_size_drug = 200\n",
    "        transformer_dropout_rate = 0.1\n",
    "        transformer_n_layer_drug = 8\n",
    "        transformer_intermediate_size_drug = 512\n",
    "        transformer_num_attention_heads_drug = 8\n",
    "        transformer_attention_probs_dropout = 0.1\n",
    "        transformer_hidden_dropout_rate = 0.1\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embDrug = Embeddings(input_dim_drug,\n",
    "                              transformer_emb_size_drug,\n",
    "                              50,\n",
    "                              transformer_dropout_rate)\n",
    "\n",
    "        self.embSide = Embeddings(input_dim_drug,\n",
    "                              transformer_emb_size_drug,\n",
    "                              50,\n",
    "                              transformer_dropout_rate)\n",
    "\n",
    "        # Transformer encoders\n",
    "        self.encoderDrug = Encoder_MultipleLayers(transformer_n_layer_drug,\n",
    "                                              transformer_emb_size_drug,\n",
    "                                              transformer_intermediate_size_drug,\n",
    "                                              transformer_num_attention_heads_drug,\n",
    "                                              transformer_attention_probs_dropout,\n",
    "                                              transformer_hidden_dropout_rate)\n",
    "\n",
    "        self.encoderSide = Encoder_MultipleLayers(transformer_n_layer_drug,\n",
    "                                              transformer_emb_size_drug,\n",
    "                                              transformer_intermediate_size_drug,\n",
    "                                              transformer_num_attention_heads_drug,\n",
    "                                              transformer_attention_probs_dropout,\n",
    "                                              transformer_hidden_dropout_rate)\n",
    "\n",
    "        # Cross Attention Encoder\n",
    "        cross_attention_n_layer = 2\n",
    "        self.crossAttentionencoder = Encoder_MultipleLayers(cross_attention_n_layer,\n",
    "                                                             transformer_emb_size_drug,\n",
    "                                                             transformer_intermediate_size_drug,\n",
    "                                                             transformer_num_attention_heads_drug,\n",
    "                                                             transformer_attention_probs_dropout,\n",
    "                                                             transformer_hidden_dropout_rate)\n",
    "\n",
    "        # Residual Fusion Layers\n",
    "        self.residual_fusion_drug = nn.Sequential(\n",
    "            nn.Linear(transformer_emb_size_drug * 2, transformer_emb_size_drug),\n",
    "            nn.LayerNorm(transformer_emb_size_drug),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(transformer_dropout_rate),\n",
    "            nn.Linear(transformer_emb_size_drug, transformer_emb_size_drug)\n",
    "        )\n",
    "        \n",
    "        self.residual_fusion_side = nn.Sequential(\n",
    "            nn.Linear(transformer_emb_size_drug * 2, transformer_emb_size_drug),\n",
    "            nn.LayerNorm(transformer_emb_size_drug),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(transformer_dropout_rate),\n",
    "            nn.Linear(transformer_emb_size_drug, transformer_emb_size_drug)\n",
    "        )\n",
    "        \n",
    "        # Gating mechanisms\n",
    "        self.gate_drug = nn.Sequential(\n",
    "            nn.Linear(transformer_emb_size_drug * 2, transformer_emb_size_drug),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.gate_side = nn.Sequential(\n",
    "            nn.Linear(transformer_emb_size_drug * 2, transformer_emb_size_drug),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(500, 200)\n",
    "        self.dropout = 0.3\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(6912, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.icnn = nn.Conv2d(1, 3, 3, padding=0)\n",
    "        self.CrossAttention = True\n",
    "\n",
    "    def forward(self, Drug, SE, DrugMask, SEMsak):\n",
    "        batch = Drug.size(0)\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Drug encoding\n",
    "        Drug = Drug.long().to(device)\n",
    "        DrugMask = DrugMask.long().to(device)\n",
    "        DrugMask = DrugMask.unsqueeze(1).unsqueeze(2)\n",
    "        DrugMask = (1.0 - DrugMask) * -10000.0\n",
    "        emb = self.embDrug(Drug)\n",
    "        encoded_layers = self.encoderDrug(emb.float(), DrugMask.float(), False)\n",
    "        x_d = encoded_layers\n",
    "\n",
    "        # Side effect encoding\n",
    "        SE = SE.long().to(device)\n",
    "        SEMsak = SEMsak.long().to(device)\n",
    "        SEMsak = SEMsak.unsqueeze(1).unsqueeze(2)\n",
    "        SEMsak = (1.0 - SEMsak) * -10000.0\n",
    "        embE = self.embSide(SE)\n",
    "        encoded_layers = self.encoderSide(embE.float(), SEMsak.float(), False)\n",
    "        x_e = encoded_layers\n",
    "\n",
    "        if self.CrossAttention:\n",
    "            x_d_original = x_d.clone()\n",
    "            x_e_original = x_e.clone()\n",
    "            combined_mask = DrugMask.float()\n",
    "            \n",
    "            cross_output = self.crossAttentionencoder([x_d.float(), x_e.float()], combined_mask, True)\n",
    "            x_d_cross = cross_output[0]\n",
    "            x_e_cross = cross_output[1]\n",
    "            \n",
    "            batch_size, seq_len, hidden_size = x_d_original.shape\n",
    "            \n",
    "            x_d_flat = x_d_original.view(-1, hidden_size)\n",
    "            x_d_cross_flat = x_d_cross.view(-1, hidden_size)\n",
    "            x_e_flat = x_e_original.view(-1, hidden_size)\n",
    "            x_e_cross_flat = x_e_cross.view(-1, hidden_size)\n",
    "            \n",
    "            x_d_concat = torch.cat([x_d_flat, x_d_cross_flat], dim=-1)\n",
    "            x_e_concat = torch.cat([x_e_flat, x_e_cross_flat], dim=-1)\n",
    "            \n",
    "            gate_d = self.gate_drug(x_d_concat)\n",
    "            gate_e = self.gate_side(x_e_concat)\n",
    "            \n",
    "            x_d_fused = self.residual_fusion_drug(x_d_concat)\n",
    "            x_e_fused = self.residual_fusion_side(x_e_concat)\n",
    "            \n",
    "            x_d = (gate_d * x_d_fused + (1 - gate_d) * x_d_flat).view(batch_size, seq_len, hidden_size)\n",
    "            x_e = (gate_e * x_e_fused + (1 - gate_e) * x_e_flat).view(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Interaction\n",
    "        d_aug = torch.unsqueeze(x_d, 2).repeat(1, 1, 50, 1)\n",
    "        e_aug = torch.unsqueeze(x_e, 1).repeat(1, 50, 1, 1)\n",
    "\n",
    "        i = d_aug * e_aug\n",
    "        i_v = i.permute(0, 3, 1, 2)\n",
    "        i_v = torch.sum(i_v, dim=1)\n",
    "        i_v = torch.unsqueeze(i_v, 1)\n",
    "        i_v = F.dropout(i_v, p=self.dropout)\n",
    "\n",
    "        f = self.icnn(i_v)\n",
    "        f = f.view(int(batch), -1)\n",
    "\n",
    "        score = self.decoder(f)\n",
    "\n",
    "        return score, Drug, SE\n",
    "\n",
    "print(\"üöÄ HSTrans model loaded!\")\n",
    "model = Trans()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üéØ 4. Training Functions\n",
    "\n",
    "Let's define the training, evaluation, and data processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_processing"
   },
   "outputs": [],
   "source": [
    "# Data processing functions\n",
    "def Extract_positive_negative_samples(DAL, addition_negative_number=''):\n",
    "    k = 0\n",
    "    interaction_target = np.zeros((DAL.shape[0] * DAL.shape[1], 3)).astype(int)\n",
    "    for i in range(DAL.shape[0]):\n",
    "        for j in range(DAL.shape[1]):\n",
    "            interaction_target[k, 0] = i\n",
    "            interaction_target[k, 1] = j\n",
    "            interaction_target[k, 2] = DAL[i, j]\n",
    "            k = k + 1\n",
    "    data_shuffle = interaction_target[interaction_target[:, 2].argsort()]\n",
    "    number_positive = len(np.nonzero(data_shuffle[:, 2])[0])\n",
    "    final_positive_sample = data_shuffle[interaction_target.shape[0] - number_positive::]\n",
    "    negative_sample = data_shuffle[0:interaction_target.shape[0] - number_positive]\n",
    "    a = np.arange(interaction_target.shape[0] - number_positive)\n",
    "    a = list(a)\n",
    "    if addition_negative_number == 'all':\n",
    "        b = random.sample(a, (interaction_target.shape[0] - number_positive))\n",
    "    else:\n",
    "        b = random.sample(a, (1 + addition_negative_number) * number_positive)\n",
    "    final_negtive_sample = negative_sample[b[0:number_positive], :]\n",
    "    addition_negative_sample = negative_sample[b[number_positive::], :]\n",
    "    final_positive_sample = np.concatenate((final_positive_sample, final_negtive_sample), axis=0)\n",
    "    return addition_negative_sample, final_positive_sample, final_negtive_sample\n",
    "\n",
    "def identify_sub(data, k):\n",
    "    print(f'üîç Extracting effective substructures for fold {k}...')\n",
    "    drug_smile = [item[1] for item in data]\n",
    "    side_id = [item[0] for item in data]\n",
    "    labels = [item[2] for item in data]\n",
    "\n",
    "    # Get SMILE-sub index\n",
    "    sub_dict = {}\n",
    "    for i in tqdm(range(len(drug_smile)), desc=\"Processing drugs\"):\n",
    "        drug_sub, mask = drug2emb_encoder(drug_smile[i])\n",
    "        drug_sub = drug_sub.tolist()\n",
    "        sub_dict[i] = drug_sub\n",
    "\n",
    "    # Save temporary file\n",
    "    with open(f'{SUB_DIR}/my_dict_{k}.pkl', 'wb') as f:\n",
    "        pickle.dump(sub_dict, f)\n",
    "    \n",
    "    with open(f'{SUB_DIR}/my_dict_{k}.pkl', 'rb') as f:\n",
    "        sub_dict = pickle.load(f)\n",
    "\n",
    "    SE_sub = np.zeros((994, 2686))\n",
    "    for j in tqdm(range(len(drug_smile)), desc=\"Building substructure matrix\"):\n",
    "        sideID = side_id[j]\n",
    "        label = float(labels[j])\n",
    "        for sub_id in sub_dict[j]:\n",
    "            if sub_id == 0:\n",
    "                continue\n",
    "            SE_sub[int(sideID)][int(sub_id)] += label\n",
    "\n",
    "    np.save(f\"{SUB_DIR}/SE_sub_{k}.npy\", SE_sub)\n",
    "    SE_sub = np.load(f\"{SUB_DIR}/SE_sub_{k}.npy\", allow_pickle=True)\n",
    "\n",
    "    n = np.sum(SE_sub)\n",
    "    SE_sum = np.sum(SE_sub, axis=1)\n",
    "    SE_p = SE_sum / n\n",
    "    Sub_sum = np.sum(SE_sub, axis=0)\n",
    "    Sub_p = Sub_sum / n\n",
    "    SE_sub_p = SE_sub / n\n",
    "\n",
    "    freq = np.zeros((994, 2686))\n",
    "    for i in tqdm(range(994), desc=\"Calculating frequencies\"):\n",
    "        for j in range(2686):\n",
    "            freq[i][j] = ((SE_sub_p[i][j] - SE_p[i] * Sub_p[j]) / (sqrt((SE_p[i] * Sub_p[j] / n)\n",
    "                                                                        * (1 - SE_p[i]) *\n",
    "                                                                        (1 - Sub_p[j])))) + 1e-5\n",
    "    np.save(f\"{SUB_DIR}/freq_{k}.npy\", freq)\n",
    "    freq = np.load(f\"{SUB_DIR}/freq_{k}.npy\", allow_pickle=True)\n",
    "    non_nan_values = freq[~np.isnan(freq)]\n",
    "    percentile_95 = np.percentile(non_nan_values, 95)\n",
    "    print(f\"üìä 95th percentile: {percentile_95:.4f}\")\n",
    "\n",
    "    l = []\n",
    "    SE_sub_index = np.zeros((994, 50))\n",
    "    for i in tqdm(range(994), desc=\"Extracting top substructures\"):\n",
    "        k_count = 0\n",
    "        sorted_indices = np.argsort(freq[i])[::-1]\n",
    "        filtered_indices = sorted_indices[freq[i][sorted_indices] > percentile_95]\n",
    "        l.append(len(filtered_indices))\n",
    "        for j in filtered_indices:\n",
    "            if k_count < 50:\n",
    "                SE_sub_index[i][k_count] = j\n",
    "                k_count = k_count + 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    np.save(f\"{SUB_DIR}/SE_sub_index_50_{k}.npy\", SE_sub_index)\n",
    "    np.save(f\"{SUB_DIR}/SE_sub_mask_50_{k}.npy\", (SE_sub_index > 0).astype(int))\n",
    "    np.save(f\"{WORK_DIR}/len_sub.npy\", l)\n",
    "    print(f\"‚úÖ Substructure extraction completed for fold {k}\")\n",
    "\n",
    "print(\"üìä Data processing functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_class"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Data_Encoder(data.Dataset):\n",
    "    def __init__(self, list_IDs, labels, df_dti, k):\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.df = df_dti\n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.list_IDs[index]\n",
    "        d = self.df.iloc[index]['Drug_smile']\n",
    "        s = int(self.df.iloc[index]['SE_id'])\n",
    "\n",
    "        d_v, input_mask_d = drug2emb_encoder(d)\n",
    "\n",
    "        # Load pre-computed side effect substructures\n",
    "        SE_index = np.load(f\"{SUB_DIR}/SE_sub_index_50_32.npy\").astype(int)\n",
    "        SE_mask = np.load(f\"{SUB_DIR}/SE_sub_mask_50_32.npy\")\n",
    "        s_v = SE_index[s, :]\n",
    "        input_mask_s = SE_mask[s, :]\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        return d_v, s_v, input_mask_d, input_mask_s, y\n",
    "\n",
    "print(\"üì¶ Dataset class loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_functions"
   },
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def loss_fun(output, label):\n",
    "    loss = torch.sum((output - label) ** 2)\n",
    "    return loss\n",
    "\n",
    "def trainfun(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    avg_loss = []\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\")\n",
    "\n",
    "    for batch_idx, (Drug, SE, DrugMask, SEMsak, Label) in enumerate(pbar):\n",
    "        Drug = Drug.to(device)\n",
    "        SE = SE.to(device)\n",
    "        DrugMask = DrugMask.to(device)\n",
    "        SEMsak = SEMsak.to(device)\n",
    "        Label = torch.FloatTensor([int(item) for item in Label]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, _, _ = model(Drug, SE, DrugMask, SEMsak)\n",
    "        pred = out.to(device)\n",
    "\n",
    "        loss = loss_fun(pred.flatten(), Label).to('cpu')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "\n",
    "    return sum(avg_loss) / len(avg_loss)\n",
    "\n",
    "def predict(model, device, test_loader):\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (Drug, SE, DrugMask, SEMsak, Label) in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "            Drug = Drug.to(device)\n",
    "            SE = SE.to(device)\n",
    "            DrugMask = DrugMask.to(device)\n",
    "            SEMsak = SEMsak.to(device)\n",
    "            Label = torch.FloatTensor([int(item) for item in Label]).to(device)\n",
    "            out, _, _ = model(Drug, SE, DrugMask, SEMsak)\n",
    "\n",
    "            location = torch.where(Label != 0)\n",
    "            pred = out[location]\n",
    "            label = Label[location]\n",
    "\n",
    "            total_preds = torch.cat((total_preds, pred.detach().cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, label.detach().cpu()), 0)\n",
    "\n",
    "    return total_labels.numpy().flatten(), total_preds.numpy().flatten()\n",
    "\n",
    "def evaluate(model, device, test_loader):\n",
    "    total_preds = torch.Tensor()\n",
    "    total_label = torch.Tensor()\n",
    "    singleDrug_auc = []\n",
    "    singleDrug_aupr = []\n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (Drug, SE, DrugMask, SEMsak, Label) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            Drug = Drug.to(device)\n",
    "            SE = SE.to(device)\n",
    "            DrugMask = DrugMask.to(device)\n",
    "            SEMsak = SEMsak.to(device)\n",
    "            Label = torch.FloatTensor([int(item) for item in Label]).to(device)\n",
    "            output, _, _ = model(Drug, SE, DrugMask, SEMsak)\n",
    "            pred = output.detach().cpu()\n",
    "            pred = torch.Tensor(pred)\n",
    "\n",
    "            total_preds = torch.cat((total_preds, pred), 0)\n",
    "            total_label = torch.cat((total_label, Label), 0)\n",
    "\n",
    "            pred = pred.numpy().flatten()\n",
    "            pred = np.where(pred > 0.5, 1, 0)\n",
    "            label = (Label.numpy().flatten() != 0).astype(int)\n",
    "            label = np.where(label != 0, 1, label)\n",
    "\n",
    "            singleDrug_auc.append(roc_auc_score(label, pred))\n",
    "            singleDrug_aupr.append(average_precision_score(label, pred))\n",
    "\n",
    "        drugAUC = sum(singleDrug_auc) / len(singleDrug_auc)\n",
    "        drugAUPR = sum(singleDrug_aupr) / len(singleDrug_aupr)\n",
    "        total_preds = total_preds.numpy()\n",
    "        total_label = total_label.numpy()\n",
    "\n",
    "        total_pre_binary = np.where(total_preds > 0.5, 1, 0)\n",
    "        label01 = np.where(total_label != 0, 1, total_label)\n",
    "\n",
    "        pre_list = total_pre_binary.tolist()\n",
    "        label_list = label01.tolist()\n",
    "\n",
    "        precision = precision_score(pre_list, label_list)\n",
    "        recall = recall_score(pre_list, label_list)\n",
    "        accuracy = accuracy_score(pre_list, label_list)\n",
    "\n",
    "        total_preds = np.where(total_preds > 0.5, 1, 0)\n",
    "        total_label = np.where(total_label != 0, 1, total_label)\n",
    "\n",
    "        pos = np.squeeze(total_preds[np.where(total_label)])\n",
    "        pos_label = np.ones(len(pos))\n",
    "\n",
    "        neg = np.squeeze(total_preds[np.where(total_label == 0)])\n",
    "        neg_label = np.zeros(len(neg))\n",
    "\n",
    "        y = np.hstack((pos, neg))\n",
    "        y_true = np.hstack((pos_label, neg_label))\n",
    "        auc_all = roc_auc_score(y_true, y)\n",
    "        aupr_all = average_precision_score(y_true, y)\n",
    "\n",
    "    return auc_all, aupr_all, drugAUC, drugAUPR, precision, recall, accuracy\n",
    "\n",
    "print(\"üèãÔ∏è Training functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_training"
   },
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def main(training_generator, testing_generator, modeling, lr, num_epoch, weight_decay, \n",
    "         log_interval, cuda_name, save_model, k, save_every=5, resume_path=None):\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print(f'üöÄ Starting training for fold {k}')\n",
    "    print(f'Model: {modeling.__name__}')\n",
    "    print(f'Learning rate: {lr}')\n",
    "    print(f'Epochs: {num_epoch}')\n",
    "    print(f'Weight decay: {weight_decay}')\n",
    "    print('='*80)\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(cuda_name if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'üñ•Ô∏è  Using device: {device}')\n",
    "\n",
    "    # Model initialization\n",
    "    model = modeling().to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'üìä Total trainable parameters: {total_params:,}')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    start_epoch = 0\n",
    "    if resume_path is not None and os.path.exists(resume_path):\n",
    "        try:\n",
    "            ckpt = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "            if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
    "                model.load_state_dict(ckpt['model_state_dict'])\n",
    "                if 'optimizer_state_dict' in ckpt:\n",
    "                    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "                start_epoch = int(ckpt.get('epoch', 0))\n",
    "                if 'random_state' in ckpt:\n",
    "                    torch.set_rng_state(ckpt['random_state'])\n",
    "                if 'numpy_random_state' in ckpt:\n",
    "                    np.random.set_state(ckpt['numpy_random_state'])\n",
    "                print(f\"üîÑ Resuming from {resume_path} at epoch {start_epoch}\")\n",
    "            else:\n",
    "                model.load_state_dict(ckpt)\n",
    "                print(f\"üîÑ Loaded model weights from legacy checkpoint {resume_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not resume from {resume_path}: {e}\")\n",
    "\n",
    "    history = []\n",
    "    train_losses = []\n",
    "\n",
    "    # Load existing history if resuming\n",
    "    metrics_file = os.path.join(RESULTS_DIR, 'train_metrics_per_epoch.json')\n",
    "    if start_epoch > 0 and os.path.exists(metrics_file):\n",
    "        try:\n",
    "            with open(metrics_file, 'r', encoding='utf-8') as f:\n",
    "                history = json.load(f)\n",
    "        except Exception:\n",
    "            history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epoch):\n",
    "        train_loss = trainfun(model=model, device=device,\n",
    "                              train_loader=training_generator,\n",
    "                              optimizer=optimizer, epoch=epoch + 1, \n",
    "                              log_interval=log_interval)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if ((epoch + 1) % save_every == 0) or (epoch == num_epoch - 1):\n",
    "            ckpt_obj = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'random_state': torch.get_rng_state(),\n",
    "                'numpy_random_state': np.random.get_state(),\n",
    "            }\n",
    "            ckpt_path = os.path.join(CHECKPOINTS_DIR, f'{k}_{epoch + 1}.pth')\n",
    "            torch.save(ckpt_obj, ckpt_path)\n",
    "            torch.save(ckpt_obj, os.path.join(CHECKPOINTS_DIR, f'latest_{k}.pth'))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_true, y_pred = predict(model=model, device=device, test_loader=testing_generator)\n",
    "        ep_mse = mse(y_true, y_pred)\n",
    "        ep_rmse = rmse(y_true, y_pred)\n",
    "        ep_scc = spearman(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epoch} - TrainLoss: {train_loss:.6f} - MSE: {ep_mse:.6f} - RMSE: {ep_rmse:.6f} - SCC: {ep_scc:.6f}\")\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': int(epoch+1),\n",
    "            'train_loss': float(train_loss),\n",
    "            'MSE': float(ep_mse),\n",
    "            'RMSE': float(ep_rmse),\n",
    "            'SCC': float(ep_scc)\n",
    "        })\n",
    "        \n",
    "        # Save history after each epoch\n",
    "        with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(history, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\nüîÆ Making predictions...\")\n",
    "    test_labels, test_preds = predict(model=model, device=device, test_loader=testing_generator)\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(PREDICT_DIR, exist_ok=True)\n",
    "    np.save(f'{PREDICT_DIR}/total_labels_{k}.npy', test_labels)\n",
    "    np.save(f'{PREDICT_DIR}/total_preds_{k}.npy', test_preds)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_MSE = mse(test_labels, test_preds)\n",
    "    test_RMSE = rmse(test_labels, test_preds)\n",
    "    test_SCC = spearman(test_labels, test_preds)\n",
    "\n",
    "    print(\"\\nüìä Evaluating performance...\")\n",
    "    auc_all, aupr_all, drugAUC, drugAUPR, precision, recall, accuracy = evaluate(\n",
    "        model=model, device=device, test_loader=testing_generator)\n",
    "\n",
    "    print(f'\\nüéØ Test Results (Regression): MSE: {test_MSE:.5f}\\tRMSE: {test_RMSE:.5f}\\tSCC: {test_SCC:.5f}')\n",
    "    print(f'üìà Classification Metrics: AUC: {auc_all:.5f}\\tAUPR: {aupr_all:.5f}\\tDrug AUC: {drugAUC:.5f}\\tDrug AUPR: {drugAUPR:.5f}')\n",
    "    print(f'üéØ Precision: {precision:.5f}\\tRecall: {recall:.5f}\\tAccuracy: {accuracy:.5f}')\n",
    "\n",
    "    # Save final metrics\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    metrics_path = os.path.join(RESULTS_DIR, f'metrics_fold_{k}.json')\n",
    "    with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'fold': k,\n",
    "            'MSE': float(test_MSE),\n",
    "            'RMSE': float(test_RMSE),\n",
    "            'SCC': float(test_SCC),\n",
    "            'AUC_all': float(auc_all),\n",
    "            'AUPR_all': float(aupr_all),\n",
    "            'AUC_drug': float(drugAUC),\n",
    "            'AUPR_drug': float(drugAUPR),\n",
    "            'Precision': float(precision),\n",
    "            'Recall': float(recall),\n",
    "            'Accuracy': float(accuracy)\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f'‚úÖ Metrics saved to {metrics_path}')\n",
    "    \n",
    "    return {\n",
    "        'MSE': test_MSE, 'RMSE': test_RMSE, 'SCC': test_SCC,\n",
    "        'AUC_all': auc_all, 'AUPR_all': aupr_all, \n",
    "        'AUC_drug': drugAUC, 'AUPR_drug': drugAUPR,\n",
    "        'Precision': precision, 'Recall': recall, 'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "print(\"üéØ Main training function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cv-training"
   },
   "source": [
    "## üîÑ 5. Cross-Validation Training\n",
    "\n",
    "Now let's set up and run the 10-fold cross-validation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_setup"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "class Config:\n",
    "    # Model parameters\n",
    "    model_type = 0  # Trans model\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    num_epoch = 100  # Reduced for Colab demo\n",
    "    log_interval = 40\n",
    "    cuda_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    save_model = True\n",
    "    batch_size = 128\n",
    "    save_every = 10\n",
    "    resume_path = None\n",
    "    \n",
    "    # Data files\n",
    "    raw_file = f'{DATA_DIR}/raw_frequency_750.mat'\n",
    "    SMILES_file = f'{DATA_DIR}/drug_SMILES_750.csv'\n",
    "    mask_mat_file = f'{DATA_DIR}/mask_mat_750.mat'\n",
    "    side_effect_label = f'{DATA_DIR}/side_effect_label_750.mat'\n",
    "    drug_side_file = f'{DATA_DIR}/drug_side.pkl'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"  Model: Trans\")\n",
    "print(f\"  Learning rate: {config.lr}\")\n",
    "print(f\"  Epochs: {config.num_epoch}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Device: {config.cuda_name}\")\n",
    "print(f\"  Weight decay: {config.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data_cv"
   },
   "outputs": [],
   "source": [
    "# Load data for cross-validation\n",
    "print(\"üìÇ Loading data...\")\n",
    "\n",
    "# Load drug-side effect interactions\n",
    "if os.path.exists(config.drug_side_file):\n",
    "    with open(config.drug_side_file, 'rb') as f:\n",
    "        drug_side = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded drug-side interactions: {drug_side.shape}\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {config.drug_side_file}\")\n",
    "    print(\"Please ensure all data files are uploaded to Google Drive\")\n",
    "\n",
    "# Load drug SMILES\n",
    "if os.path.exists(config.SMILES_file):\n",
    "    drug_dict, drug_smile = load_drug_smile(config.SMILES_file)\n",
    "    print(f\"‚úÖ Loaded {len(drug_smile)} drug SMILES\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {config.SMILES_file}\")\n",
    "\n",
    "# Extract positive and negative samples\n",
    "print(\"\\nüîç Extracting positive and negative samples...\")\n",
    "addition_negative_sample, final_positive_sample, final_negative_sample = Extract_positive_negative_samples(\n",
    "    drug_side, addition_negative_number='all')\n",
    "\n",
    "addition_negative_sample = np.vstack((addition_negative_sample, final_negative_sample))\n",
    "final_sample = final_positive_sample\n",
    "X = final_sample[:, 0::]\n",
    "final_target = final_sample[:, final_sample.shape[1] - 1]\n",
    "y = final_target\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(final_positive_sample)} positive samples\")\n",
    "print(f\"‚úÖ Extracted {len(addition_negative_sample)} negative samples\")\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "data = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    data_x.append((X[i, 1], X[i, 0]))\n",
    "    data_y.append((int(float(X[i, 2]))))\n",
    "    data.append((X[i, 1], drug_smile[X[i, 0]], X[i, 2]))\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(data)} samples for cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "substructure_preprocessing"
   },
   "outputs": [],
   "source": [
    "# Precompute substructures (run once)\n",
    "print(\"üß© Computing substructure indices...\")\n",
    "identify_sub(data, 0)\n",
    "\n",
    "# Copy the computed indices for all folds\n",
    "import shutil\n",
    "for fold in range(1, 10):\n",
    "    shutil.copy(f'{SUB_DIR}/SE_sub_index_50_0.npy', f'{SUB_DIR}/SE_sub_index_50_{fold}.npy')\n",
    "    shutil.copy(f'{SUB_DIR}/SE_sub_mask_50_0.npy', f'{SUB_DIR}/SE_sub_mask_50_{fold}.npy')\n",
    "\n",
    "# Also create the 32 index files that the code expects\n",
    "shutil.copy(f'{SUB_DIR}/SE_sub_index_50_0.npy', f'{SUB_DIR}/SE_sub_index_50_32.npy')\n",
    "shutil.copy(f'{SUB_DIR}/SE_sub_mask_50_0.npy', f'{SUB_DIR}/SE_sub_mask_50_32.npy')\n",
    "\n",
    "print(\"‚úÖ Substructure indices prepared for all folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_cv_training"
   },
   "outputs": [],
   "source": [
    "# Run cross-validation training\n",
    "fold_results = []\n",
    "modeling = Trans\n",
    "\n",
    "# Set up cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "params = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "print(\"\\nüöÄ Starting 10-fold cross-validation training...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(data_x, data_y)):\n",
    "    print(f\"\\nüìÅ Fold {fold + 1}/10\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Split data\n",
    "    data_train = np.array(data)[train_idx]\n",
    "    data_test = np.array(data)[test_idx]\n",
    "\n",
    "    # Create DataFrames\n",
    "    df_train = pd.DataFrame(data=data_train.tolist(), columns=['SE_id', 'Drug_smile', 'Label'])\n",
    "    df_test = pd.DataFrame(data=data_test.tolist(), columns=['SE_id', 'Drug_smile', 'Label'])\n",
    "\n",
    "    print(f\"üìä Train samples: {len(df_train)}, Test samples: {len(df_test)}\")\n",
    "    print(f\"üìà Train positives: {df_train['Label'].sum()}, Test positives: {df_test['Label'].sum()}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    training_set = Data_Encoder(df_train.index.values, df_train.Label.values, df_train, fold)\n",
    "    testing_set = Data_Encoder(df_test.index.values, df_test.Label.values, df_test, fold)\n",
    "\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "    testing_generator = torch.utils.data.DataLoader(testing_set, **params)\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        result = main(\n",
    "            training_generator=training_generator,\n",
    "            testing_generator=testing_generator,\n",
    "            modeling=modeling,\n",
    "            lr=config.lr,\n",
    "            num_epoch=config.num_epoch,\n",
    "            weight_decay=config.weight_decay,\n",
    "            log_interval=config.log_interval,\n",
    "            cuda_name=config.cuda_name,\n",
    "            save_model=config.save_model,\n",
    "            k=fold,\n",
    "            save_every=config.save_every,\n",
    "            resume_path=config.resume_path\n",
    "        )\n",
    "        \n",
    "        result['fold'] = fold\n",
    "        fold_results.append(result)\n",
    "        print(f\"‚úÖ Fold {fold + 1} completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fold {fold + 1} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\nüéâ Cross-validation training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìä 6. Results Analysis\n",
    "\n",
    "Let's analyze and visualize the results from all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aggregate_results"
   },
   "outputs": [],
   "source": [
    "# Aggregate results across folds\n",
    "if fold_results:\n",
    "    print(\"üìä Aggregating results across all folds...\")\n",
    "    \n",
    "    # Calculate mean and std for each metric\n",
    "    metrics = ['MSE', 'RMSE', 'SCC', 'AUC_all', 'AUPR_all', 'AUC_drug', 'AUPR_drug', 'Precision', 'Recall', 'Accuracy']\n",
    "    summary = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = [result[metric] for result in fold_results if metric in result]\n",
    "        if values:\n",
    "            summary[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'values': values\n",
    "            }\n",
    "    \n",
    "    # Create summary table\n",
    "    print(\"\\nüéØ Final Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    for metric, stats in summary.items():\n",
    "        print(f\"{metric:15s}: {stats['mean']:.4f} ¬± {stats['std']:.4f}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = os.path.join(RESULTS_DIR, 'cv_summary.json')\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nüíæ Summary saved to {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results to analyze. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_results"
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if fold_results:\n",
    "    # Create visualization plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('HSTrans Cross-Validation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Regression metrics\n",
    "    reg_metrics = ['MSE', 'RMSE', 'SCC']\n",
    "    for i, metric in enumerate(reg_metrics):\n",
    "        values = [result[metric] for result in fold_results if metric in result]\n",
    "        axes[0, i].boxplot(values, labels=[metric])\n",
    "        axes[0, i].set_title(f'{metric} Across Folds')\n",
    "        axes[0, i].set_ylabel(metric)\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Classification metrics\n",
    "    cls_metrics = ['AUC_all', 'AUPR_all', 'Accuracy']\n",
    "    for i, metric in enumerate(cls_metrics):\n",
    "        values = [result[metric] for result in fold_results if metric in result]\n",
    "        axes[1, i].boxplot(values, labels=[metric])\n",
    "        axes[1, i].set_title(f'{metric} Across Folds')\n",
    "        axes[1, i].set_ylabel(metric)\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'cv_results_boxplot.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training curves (if available)\n",
    "    metrics_file = os.path.join(RESULTS_DIR, 'train_metrics_per_epoch.json')\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r', encoding='utf-8') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        if history:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            epochs = [h['epoch'] for h in history]\n",
    "            train_losses = [h['train_loss'] for h in history]\n",
    "            mse_values = [h['MSE'] for h in history]\n",
    "            scc_values = [h['SCC'] for h in history]\n",
    "            \n",
    "            ax1.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            ax2.plot(epochs, mse_values, 'r-', label='MSE')\n",
    "            ax2.plot(epochs, scc_values, 'g-', label='SCC')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Metric Value')\n",
    "            ax2.set_title('Validation Metrics')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(RESULTS_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results to visualize. Please run the training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Download results to Google Drive\n",
    "import shutil\n",
    "\n",
    "# Create results archive\n",
    "archive_path = '/content/HSTrans_Results.zip'\n",
    "shutil.make_archive('/content/HSTrans_Results', 'zip', WORK_DIR)\n",
    "\n",
    "# Copy to Google Drive\n",
    "drive_results_path = '/content/drive/MyDrive/HSTrans_Results.zip'\n",
    "shutil.copy(archive_path, drive_results_path)\n",
    "\n",
    "print(f\"‚úÖ Results archived to: {archive_path}\")\n",
    "print(f\"‚úÖ Results copied to Google Drive: {drive_results_path}\")\n",
    "print(\"\\nüìÅ You can now download the results from your Google Drive!\")\n",
    "\n",
    "# List all saved files\n",
    "print(\"\\nüìã Saved files:\")\n",
    "for root, dirs, files in os.walk(RESULTS_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(('.json', '.png', '.npy')):\n",
    "            print(f\"  üìÑ {os.path.join(root, file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### üìã What We Accomplished\n",
    "\n",
    "1. **‚úÖ Setup & Dependencies**: Installed all required packages and set up environment\n",
    "2. **‚úÖ Data Preparation**: Loaded and processed drug-side effect interaction data\n",
    "3. **‚úÖ Model Architecture**: Implemented HSTrans Transformer model with cross-attention\n",
    "4. **‚úÖ Training Pipeline**: Created comprehensive training functions with checkpointing\n",
    "5. **‚úÖ Cross-Validation**: Ran 10-fold cross-validation training\n",
    "6. **‚úÖ Results Analysis**: Visualized and aggregated performance metrics\n",
    "\n",
    "### üìä Key Features of This Colab Notebook\n",
    "\n",
    "- **üñ•Ô∏è GPU Support**: Automatic GPU detection and utilization\n",
    "- **üíæ Checkpointing**: Save/resume training progress\n",
    "- **üìà Progress Tracking**: Real-time training progress with tqdm\n",
    "- **üîÑ Cross-Validation**: Robust 10-fold CV for reliable evaluation\n",
    "- **üìä Visualization**: Comprehensive result visualization\n",
    "- **‚òÅÔ∏è Cloud Storage**: Automatic backup to Google Drive\n",
    "\n",
    "### üéØ Model Performance\n",
    "\n",
    "The HSTrans model uses:\n",
    "- **Cross-attention mechanism** for drug-side effect interaction\n",
    "- **Substructure encoding** for molecular representation\n",
    "- **Residual fusion** with gated mechanisms\n",
    "- **Multi-layer Transformer** architecture\n",
    "\n",
    "### üîß How to Use This Notebook\n",
    "\n",
    "1. **Upload your data files** to the data directory or update the `download_data()` function\n",
    "2. **Configure training parameters** in the `Config` class\n",
    "3. **Run all cells sequentially** to complete training\n",
    "4. **Check results** in the generated plots and saved files\n",
    "5. **Download results** from Google Drive\n",
    "\n",
    "### üìû Need Help?\n",
    "\n",
    "- Check **GPU availability** at the beginning\n",
    "- Ensure all **data files are uploaded** correctly\n",
    "- Monitor **memory usage** during training\n",
    "- Use **checkpoint resuming** if training gets interrupted\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_type": "standard",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
