{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Drug Side Effect Prediction with HSTrans\n",
    "\n",
    "This notebook clones the Drug-Side-Effect repository and runs the training for the HSTrans model.\n",
    "\n",
    "## Repository: https://github.com/tuanha1305/Drug-Side-Effect.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/tuanha1305/Drug-Side-Effect.git\n",
    "\n",
    "# Change to the repository directory\n",
    "%cd Drug-Side-Effect\n",
    "\n",
    "# List the contents\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_requirements"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch>=1.9.0 numpy>=1.19.0 pandas>=1.2.0 scipy>=1.6.0 scikit-learn>=0.24.0 matplotlib>=3.3.0\n",
    "\n",
    "# Install RDKit (for chemical informatics)\n",
    "!pip install rdkit-pypi\n",
    "\n",
    "# Install subword-nmt\n",
    "!pip install subword-nmt\n",
    "\n",
    "# Install networkx\n",
    "!pip install networkx>=2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_files"
   },
   "source": [
    "## 3. Check Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Check if data files exist\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_files = [\n",
    "    'data/drug_side.pkl',\n",
    "    'data/drug_SMILES_750.csv',\n",
    "    'data/raw_frequency_750.mat',\n",
    "    'data/side_effect_label_750.mat',\n",
    "    'data/subword_units_map_chembl_freq_1500.csv'\n",
    "]\n",
    "\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✓ {file} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {file} missing\")\n",
    "\n",
    "# Check the main Python files\n",
    "python_files = ['main.py', 'Net.py', 'Encoder.py', 'utils.py', 'smiles2vector.py']\n",
    "for file in python_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✓ {file} exists\")\n",
    "    else:\n",
    "        print(f\"✗ {file} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_params"
   },
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_parameters"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "import os\n",
    "\n",
    "# Set device (use GPU if available, otherwise CPU)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "log_interval = 40\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Weight decay: {weight_decay}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('predictResult', exist_ok=True)\n",
    "os.makedirs('data/sub', exist_ok=True)\n",
    "\n",
    "print(\"Directories created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_training"
   },
   "source": [
    "## 5. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execute_training"
   },
   "outputs": [],
   "source": [
    "# Run the training script with specified parameters\n",
    "!python main.py \\\n",
    "    --model 0 \\\n",
    "    --lr {learning_rate} \\\n",
    "    --wd {weight_decay} \\\n",
    "    --epoch {num_epochs} \\\n",
    "    --log_interval {log_interval} \\\n",
    "    --cuda_name {device}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor_training"
   },
   "source": [
    "## 6. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_progress"
   },
   "outputs": [],
   "source": [
    "# Check training progress by looking at the metrics file\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    with open('results/train_metrics_per_epoch.json', 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    print(\"Training Metrics:\")\n",
    "    print(df_metrics.tail())  # Show last few epochs\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(df_metrics['epoch'], df_metrics['train_loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(df_metrics['epoch'], df_metrics['MSE'])\n",
    "    plt.title('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(df_metrics['epoch'], df_metrics['RMSE'])\n",
    "    plt.title('RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Training metrics file not found yet. Training may still be in progress.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_results"
   },
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "# Display final results\n",
    "try:\n",
    "    with open('results/metrics_original.json', 'r') as f:\n",
    "        final_metrics = json.load(f)\n",
    "    \n",
    "    print(\"Final Training Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric, value in final_metrics.items():\n",
    "        print(f\"{metric}: {value:.5f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Results file not found yet. Training may still be running.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_checkpoints"
   },
   "source": [
    "## 8. Check Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_checkpoints"
   },
   "outputs": [],
   "source": [
    "# List saved model checkpoints\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'checkpoints'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "    checkpoints.sort()\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for ckpt in checkpoints:\n",
    "        ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
    "        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n",
    "        print(f\"  {ckpt} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"No checkpoints directory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_results"
   },
   "source": [
    "## 9. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Zip and download results\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Create a zip file with results\n",
    "with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n",
    "    # Add results\n",
    "    if os.path.exists('results'):\n",
    "        for root, dirs, files in os.walk('results'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), '.'))\n",
    "    \n",
    "    # Add checkpoints\n",
    "    if os.path.exists('checkpoints'):\n",
    "        for root, dirs, files in os.walk('checkpoints'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), '.'))\n",
    "    \n",
    "    # Add predictions\n",
    "    if os.path.exists('predictResult'):\n",
    "        for root, dirs, files in os.walk('predictResult'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), '.'))\n",
    "\n",
    "print(\"Results zipped successfully!\")\n",
    "print(\"File size:\", os.path.getsize('training_results.zip') / (1024*1024), \"MB\")\n",
    "\n",
    "# Download the zip file (uncomment the line below to download)\n",
    "# files.download('training_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 10. Training Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Cloned the Drug-Side-Effect repository\n",
    "2. ✅ Installed all required dependencies\n",
    "3. ✅ Set up the training environment\n",
    "4. ✅ Configured training parameters\n",
    "5. ✅ Ran the HSTrans model training\n",
    "6. ✅ Monitored training progress\n",
    "7. ✅ Displayed final results\n",
    "8. ✅ Listed saved checkpoints\n",
    "9. ✅ Prepared results for download\n",
    "\n",
    "### Model Information:\n",
    "- **Architecture**: HSTrans (Hierarchical Transformer)\n",
    "- **Task**: Drug-Drug Side Effect Prediction\n",
    "- **Input**: Drug SMILES sequences and side effect information\n",
    "- **Output**: Side effect probability predictions\n",
    "\n",
    "### Key Metrics:\n",
    "- **MSE**: Mean Squared Error (lower is better)\n",
    "- **RMSE**: Root Mean Squared Error (lower is better)\n",
    "- **SCC**: Spearman Correlation Coefficient (higher is better)\n",
    "- **AUC**: Area Under Curve (higher is better)\n",
    "- **AUPR**: Area Under Precision-Recall Curve (higher is better)\n",
    "\n",
    "### Files Generated:\n",
    "- `checkpoints/`: Model checkpoints (.pth files)\n",
    "- `results/`: Training metrics and final results\n",
    "- `predictResult/`: Model predictions\n",
    "- `data/sub/`: Substructure analysis files\n",
    "\n",
    "To continue training from a checkpoint, you can use the `--resume` parameter:\n",
    "```bash\n",
    "python main.py --resume checkpoints/latest_0.pth --lr 1e-4 --epoch 200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
